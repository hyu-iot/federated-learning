{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 DATASET SPLIT ##\n",
    "\n",
    "Total_Trainset = 50000, Total_Testset = 10000\n",
    "\n",
    "### Centralized ###\n",
    "\n",
    "Trainset = 45000, Valset= 5000, Testset = 10000\n",
    "\n",
    "### Federated ###\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "Trainset = 4500, Valset= 500, (Each client)\n",
    "Testset = 10000 (Aggregated Model)\n",
    "\n",
    "metrics_distributed 가 각 client에서 돌린 결과 평균낸 accuracy\n",
    "metrics_centralized 가 모은 model test한 결과 나온 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.models\n",
    "\n",
    "import os\n",
    "# 여기서는 10개로 나눈 것 중, 하나만 train 함.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "import flwr.common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda using PyTorch 2.0.0 and Flower 1.4.0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "client_resources = None\n",
    "if DEVICE.type == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    ")\n",
    "trainset = CIFAR10(\"../dataset\", train=True, download=True, transform=transform)\n",
    "testset = CIFAR10(\"../dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "lengths = [45000, 5000]\n",
    "split_trainset, valset = random_split(trainset, lengths, torch.Generator().manual_seed(42)) \n",
    "    \n",
    "# 45000\n",
    "full_split_trainloader = DataLoader(split_trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# 5000\n",
    "full_valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# 10000\n",
    "full_testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    partition_size = len(trainset) // NUM_CLIENTS\n",
    "    lengths = [partition_size] * NUM_CLIENTS\n",
    "    datasets= random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for idx, ds in enumerate(datasets):\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        # Always splits in the same way.\n",
    "\n",
    "        # Count Distribution of data\n",
    "        # print(dict(Counter(ds_train.targets)))\n",
    "        # custom_subset(ds_train)\n",
    "        # arr = []\n",
    "        # print(\"ds_train\", idx)\n",
    "        # for a in ds_train:\n",
    "        #     arr.append(a[1])\n",
    "        # for i in range(10):\n",
    "        #     print(i, \":\", arr.count(i))\n",
    "        # Data is not perfectly distributed\n",
    "\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 16 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models = torchvision.models.list_models()\n",
    "# all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torchvision.models.densenet121(num_classes=10).to(DEVICE)\n",
    "# net = torchvision.models.resnet18(num_classes=10).to(DEVICE)\n",
    "# net = torchvision.models.resnet34(num_classes=10).to(DEVICE)\n",
    "# net = torchvision.models.resnet50(num_classes=10).to(DEVICE)\n",
    "\n",
    "# net = torchvision.models.googlenet(num_classes=10).to(DEVICE)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, trainloader, valloader):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "    \n",
    "def client_fn(cid: str) -> FlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    # net = Net().to(DEVICE)\n",
    "\n",
    "    # Load data (CIFAR-10)\n",
    "    # Note: each client gets a different trainloader/valloader, so each client\n",
    "    # will train and evaluate on their own unique data\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return FlowerClient(net, trainloader, valloader)\n",
    "    \n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "# The `evaluate` function will be by Flower called after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: fl.common.NDArrays,\n",
    "    config: Dict[str, fl.common.Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "    # net = Net().to(DEVICE)\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, full_testloader) # Test total test_loader\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # 'Densenet121' : \"densenet121\", # fails\n",
    "    # 'Resnet18' : \"resnet18\", # fails\n",
    "    # 'Resnet34' : \"resnet34\", # fails\n",
    "    # 'Resnet50' : \"resnet50\", # fails\n",
    "    'VGG11' : \"vgg11\", \n",
    "    'VGG13' : \"vgg13\", \n",
    "    'VGG16' : \"vgg16\", \n",
    "    'VGG19' : \"vgg19\",\n",
    "    # 'Googlenet' : \"googlenet\" # fails\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centralized: VGG19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-05-04 11:29:26,040 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated: VGG19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 11:29:29,788\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "INFO flwr 2023-05-04 11:29:30,351 | app.py:180 | Flower VCE: Ray initialized with resources: {'node:172.17.0.2': 1.0, 'accelerator_type:RTX': 1.0, 'memory': 48849572660.0, 'GPU': 1.0, 'object_store_memory': 24424786329.0, 'CPU': 16.0}\n",
      "INFO flwr 2023-05-04 11:29:30,352 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-05-04 11:29:30,352 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-05-04 11:29:34,161 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2023-05-04 11:29:34,162 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-05-04 11:29:36,304 | server.py:91 | initial parameters (loss, other metrics): 0.07207234513759612, {'accuracy': 0.1016}\n",
      "INFO flwr 2023-05-04 11:29:36,304 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-05-04 11:29:36,305 | server.py:218 | fit_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07207234513759612 / accuracy 0.1016\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7m15s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[2m\u001b[1m\u001b[33m(autoscaler +7m15s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 1.0, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-05-04 11:30:42,184 | server.py:232 | fit_round 1 received 10 results and 0 failures\n",
      "WARNING flwr 2023-05-04 11:30:45,057 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
      "INFO flwr 2023-05-04 11:30:47,211 | server.py:119 | fit progress: (1, 0.07178377723693848, {'accuracy': 0.1124}, 70.9063359121792)\n",
      "DEBUG flwr 2023-05-04 11:30:47,212 | server.py:168 | evaluate_round 1: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07178377723693848 / accuracy 0.1124\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=738310)\u001b[0m Accuracy: 0.12\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=738427)\u001b[0m Accuracy: 0.094\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=738541)\u001b[0m Accuracy: 0.138\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=738662)\u001b[0m Accuracy: 0.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-05-04 11:31:03,451 | server.py:182 | evaluate_round 1 received 5 results and 0 failures\n",
      "INFO flwr 2023-05-04 11:31:03,452 | server.py:147 | FL finished in 87.14687350718305\n",
      "INFO flwr 2023-05-04 11:31:03,693 | app.py:218 | app_fit: losses_distributed [(1, 0.07337334823608399)]\n",
      "INFO flwr 2023-05-04 11:31:03,694 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2023-05-04 11:31:03,694 | app.py:220 | app_fit: metrics_distributed {'accuracy': [(1, 0.11879999999999999)]}\n",
      "INFO flwr 2023-05-04 11:31:03,694 | app.py:221 | app_fit: losses_centralized [(0, 0.07207234513759612), (1, 0.07178377723693848)]\n",
      "INFO flwr 2023-05-04 11:31:03,694 | app.py:222 | app_fit: metrics_centralized {'accuracy': [(0, 0.1016), (1, 0.1124)]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=738782)\u001b[0m Accuracy: 0.116\n",
      "['accuracy']\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.DataFrame()\n",
    "\n",
    "for key, value in models.items():\n",
    "    net = getattr(torchvision.models, value)(num_classes=10).to(DEVICE)\n",
    "    ######## Centralized #########\n",
    "    print(\"Centralized: \" + key)\n",
    "    trained_path = \"../dataset/trained_centralized_\"+key+\".pkl\"\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        train(net, full_split_trainloader, 1)\n",
    "        loss, accuracy = test(net, full_testloader) # Federated aggregation model tests full_testloader\n",
    "        print(f\"{key} Epoch {epoch+1}: validation loss {loss}, accuracy {accuracy}\")\n",
    "        df_centralized = pd.DataFrame()\n",
    "        df_centralized['round'] = epoch+1,\n",
    "        df_centralized['Centralized'] = 'Centralized',\n",
    "        df_centralized['model'] = key\n",
    "        df_centralized['c_loss'] = loss,\n",
    "        df_centralized['d_loss'] = 0.0,\n",
    "        df_centralized['c_accuracy'] = accuracy,\n",
    "        df_centralized['d_accuracy'] = 0.0\n",
    "        df_final = pd.concat([df_final, df_centralized], axis=0)\n",
    "        df_centralized.to_csv('../result/result_c_'+key+'.csv', index=False)\n",
    "    torch.save(net.state_dict(), trained_path)\n",
    "\n",
    "    ######## Federated ######## \n",
    "    # Reset net\n",
    "    net = getattr(torchvision.models, value)(num_classes=10).to(DEVICE) \n",
    "    print(\"Federated: \" + key)\n",
    "    fedavg = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=10,\n",
    "        min_evaluate_clients=5,\n",
    "        min_available_clients=10,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,   # aggregate evaluation of local model\n",
    "        evaluate_fn=evaluate,   # evaluate global model\n",
    "    )  \n",
    "    hist = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        config=fl.server.ServerConfig(num_rounds=EPOCH),\n",
    "        strategy=fedavg,\n",
    "        client_resources=client_resources,\n",
    "    ) \n",
    "\n",
    "    df_federated = pd.DataFrame()\n",
    "    df_federated['round'] = [i for i in range(1, EPOCH + 1)]\n",
    "    df_federated['Centralized'] = \"Federated\" \n",
    "    df_federated['model'] = key\n",
    "\n",
    "    # centralized metrics\n",
    "    metrics_cen = list(hist.metrics_centralized.keys())\n",
    "    metrics_dis = list(hist.metrics_distributed.keys())\n",
    "\n",
    "    for metric in metrics_cen:\n",
    "        df_federated[f\"c_{metric}\"] = [h[1] for h in hist.metrics_centralized[metric][1:]]\n",
    "    for metric in metrics_dis:\n",
    "        df_federated[f\"d_{metric}\"] = [h[1] for h in hist.metrics_distributed[metric]]\n",
    "    \n",
    "    df_federated.to_csv('../result/result_f_'+key+'.csv', index=False)\n",
    "\n",
    "    df_final = pd.concat([df_final, df_federated], axis=0)\n",
    "\n",
    "df_final.to_csv('../result/result_total.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
