{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYqlqcKQvIGA"
      },
      "source": [
        "# Strategies in Federated Learning\n",
        "\n",
        "Welcome to the next part of the federated learning tutorial. In previous parts of this tutorial, we introduced federated learning with PyTorch and Flower ([part 1](https://flower.dev/docs/tutorial/Flower-1-Intro-to-FL-PyTorch.html)).\n",
        "\n",
        "In this notebook, we'll begin to customize the federated learning system we built in the introductory notebook (again, using [Flower](https://flower.dev/) and [PyTorch](https://pytorch.org/)).\n",
        "\n",
        "> [Star Flower on GitHub](https://github.com/adap/flower) ⭐️ and join the Flower community on Slack to connect, ask questions, and get help: [Join Slack](https://flower.dev/join-slack) 🌼 We'd love to hear from you in the `#introductions` channel! And if anything is unclear, head over to the `#questions` channel.\n",
        "\n",
        "Let's move beyond FedAvg with Flower Strategies!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfOlhr8HvIGH"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Before we begin with the actual code, let's make sure that we have everything we need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WqB5yu_vIGI"
      },
      "source": [
        "### Installing dependencies\n",
        "\n",
        "First, we install the necessary packages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrv4Alp5vIGK"
      },
      "source": [
        "Now that we have all dependencies installed, we can import everything we need for this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BeNMqAFdvIGL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda using PyTorch 2.0.0 and Flower 1.4.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import flwr as fl\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Va5Hkg7vIGM"
      },
      "source": [
        "It is possible to switch to a runtime that has GPU acceleration enabled (on Google Colab: `Runtime > Change runtime type > Hardware acclerator: GPU > Save`). Note, however, that Google Colab is not always able to offer GPU acceleration. If you see an error related to GPU availability in one of the following sections, consider switching back to CPU-based execution by setting `DEVICE = torch.device(\"cpu\")`. If the runtime has GPU acceleration enabled, you should see the output `Training on cuda`, otherwise it'll say `Training on cpu`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cnWUQ8zvIGN"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "Let's now load the CIFAR-10 training and test set, partition them into ten smaller datasets (each split into training and validation set), and wrap everything in their own `DataLoader`. We introduce a new parameter `num_clients` which allows us to call `load_datasets` with different numbers of clients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R2Iow-0GvIGO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_CLIENTS = 10\n",
        "\n",
        "\n",
        "def load_datasets(num_clients: int):\n",
        "    # Download and transform CIFAR-10 (train and test)\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "    )\n",
        "    trainset = CIFAR10(\"../dataset\", train=True, download=True, transform=transform)\n",
        "    testset = CIFAR10(\"../dataset\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
        "    partition_size = len(trainset) // num_clients\n",
        "    lengths = [partition_size] * num_clients\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = len(ds) // 10  # 10 % validation set\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=32, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=32))\n",
        "    testloader = DataLoader(testset, batch_size=32)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ7wncx7vIGQ"
      },
      "source": [
        "### Model training/evaluation\n",
        "\n",
        "Let's continue with the usual model definition (including `set_parameters` and `get_parameters`), training and test functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0nDkhdltvIGR"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(net, trainloader, epochs: int):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(net(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sogq2wrJvIGR"
      },
      "source": [
        "### Flower client\n",
        "\n",
        "To implement the Flower client, we (again) create a subclass of `flwr.client.NumPyClient` and implement the three methods `get_parameters`, `fit`, and `evaluate`. Here, we also pass the `cid` to the client and use it log additional details:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CSGDFB29vIGS"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, net, trainloader, valloader):\n",
        "        self.cid = cid\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        print(f\"[Client {self.cid}] get_parameters\")\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        train(self.net, self.trainloader, epochs=1)\n",
        "        return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "\n",
        "def client_fn(cid) -> FlowerClient:\n",
        "    net = Net().to(DEVICE)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "    return FlowerClient(cid, net, trainloader, valloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJptU1aJvIGT"
      },
      "source": [
        "## Strategy customization\n",
        "\n",
        "So far, everything should look familiar if you've worked through the introductory notebook. With that, we're ready to introduce a number of new features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz82qnPHvIGT"
      },
      "source": [
        "### Server-side parameter **initialization**\n",
        "\n",
        "Flower, by default, initializes the global model by asking one random client for the initial parameters. In many cases, we want more control over parameter initialization though. Flower therefore allows you to directly pass the initial parameters to the Strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zBRN_LfxvIGT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-05-04 11:34:14,320 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
            "2023-05-04 11:34:15,934\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "INFO flwr 2023-05-04 11:34:16,492 | app.py:180 | Flower VCE: Ray initialized with resources: {'object_store_memory': 15863128473.0, 'node:172.17.0.2': 1.0, 'CPU': 16.0, 'memory': 31726256948.0, 'accelerator_type:RTX': 1.0, 'GPU': 1.0}\n",
            "INFO flwr 2023-05-04 11:34:16,493 | server.py:86 | Initializing global parameters\n",
            "INFO flwr 2023-05-04 11:34:16,493 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO flwr 2023-05-04 11:34:16,493 | server.py:88 | Evaluating initial parameters\n",
            "INFO flwr 2023-05-04 11:34:16,493 | server.py:101 | FL starting\n",
            "DEBUG flwr 2023-05-04 11:34:16,494 | server.py:218 | fit_round 1: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=744014)\u001b[0m [Client 4] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=744014)\u001b[0m Epoch 1: train loss 0.06470978260040283, accuracy 0.24222222222222223\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=744130)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=744130)\u001b[0m Epoch 1: train loss 0.06438679993152618, accuracy 0.2351111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=744230)\u001b[0m [Client 8] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=744230)\u001b[0m Epoch 1: train loss 0.06375786662101746, accuracy 0.24622222222222223\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     client_resources \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mnum_gpus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m}\n\u001b[1;32m     19\u001b[0m \u001b[39m# Start simulation\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m fl\u001b[39m.\u001b[39;49msimulation\u001b[39m.\u001b[39;49mstart_simulation(\n\u001b[1;32m     21\u001b[0m     client_fn\u001b[39m=\u001b[39;49mclient_fn,\n\u001b[1;32m     22\u001b[0m     num_clients\u001b[39m=\u001b[39;49mNUM_CLIENTS,\n\u001b[1;32m     23\u001b[0m     config\u001b[39m=\u001b[39;49mfl\u001b[39m.\u001b[39;49mserver\u001b[39m.\u001b[39;49mServerConfig(num_rounds\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m),  \u001b[39m# Just three rounds\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     25\u001b[0m     client_resources\u001b[39m=\u001b[39;49mclient_resources,\n\u001b[1;32m     26\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/simulation/app.py:197\u001b[0m, in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised)\u001b[0m\n\u001b[1;32m    194\u001b[0m     initialized_server\u001b[39m.\u001b[39mclient_manager()\u001b[39m.\u001b[39mregister(client\u001b[39m=\u001b[39mclient_proxy)\n\u001b[1;32m    196\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m hist \u001b[39m=\u001b[39m _fl(\n\u001b[1;32m    198\u001b[0m     server\u001b[39m=\u001b[39;49minitialized_server,\n\u001b[1;32m    199\u001b[0m     config\u001b[39m=\u001b[39;49minitialized_config,\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m event(EventType\u001b[39m.\u001b[39mSTART_SIMULATION_LEAVE)\n\u001b[1;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m hist\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/app.py:217\u001b[0m, in \u001b[0;36m_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fl\u001b[39m(\n\u001b[1;32m    213\u001b[0m     server: Server,\n\u001b[1;32m    214\u001b[0m     config: ServerConfig,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m History:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# Fit model\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     hist \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49mfit(num_rounds\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rounds, timeout\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mround_timeout)\n\u001b[1;32m    218\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: losses_distributed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mlosses_distributed))\n\u001b[1;32m    219\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: metrics_distributed_fit \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mmetrics_distributed_fit))\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/server.py:106\u001b[0m, in \u001b[0;36mServer.fit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m start_time \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[1;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m current_round \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_rounds \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    105\u001b[0m     \u001b[39m# Train model and replace previous global model\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     res_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_round(server_round\u001b[39m=\u001b[39;49mcurrent_round, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m res_fit:\n\u001b[1;32m    108\u001b[0m         parameters_prime, fit_metrics, _ \u001b[39m=\u001b[39m res_fit  \u001b[39m# fit_metrics_aggregated\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/server.py:227\u001b[0m, in \u001b[0;36mServer.fit_round\u001b[0;34m(self, server_round, timeout)\u001b[0m\n\u001b[1;32m    218\u001b[0m log(\n\u001b[1;32m    219\u001b[0m     DEBUG,\n\u001b[1;32m    220\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfit_round \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: strategy sampled \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m clients (out of \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client_manager\u001b[39m.\u001b[39mnum_available(),\n\u001b[1;32m    224\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[39m# Collect `fit` results from all clients participating in this round\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m results, failures \u001b[39m=\u001b[39m fit_clients(\n\u001b[1;32m    228\u001b[0m     client_instructions\u001b[39m=\u001b[39;49mclient_instructions,\n\u001b[1;32m    229\u001b[0m     max_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_workers,\n\u001b[1;32m    230\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m log(\n\u001b[1;32m    233\u001b[0m     DEBUG,\n\u001b[1;32m    234\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfit_round \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m received \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m results and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m failures\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39mlen\u001b[39m(failures),\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    240\u001b[0m \u001b[39m# Aggregate training results\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/server.py:334\u001b[0m, in \u001b[0;36mfit_clients\u001b[0;34m(client_instructions, max_workers, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mmax_workers) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m    330\u001b[0m     submitted_fs \u001b[39m=\u001b[39m {\n\u001b[1;32m    331\u001b[0m         executor\u001b[39m.\u001b[39msubmit(fit_client, client_proxy, ins, timeout)\n\u001b[1;32m    332\u001b[0m         \u001b[39mfor\u001b[39;00m client_proxy, ins \u001b[39min\u001b[39;00m client_instructions\n\u001b[1;32m    333\u001b[0m     }\n\u001b[0;32m--> 334\u001b[0m     finished_fs, _ \u001b[39m=\u001b[39m concurrent\u001b[39m.\u001b[39;49mfutures\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    335\u001b[0m         fs\u001b[39m=\u001b[39;49msubmitted_fs,\n\u001b[1;32m    336\u001b[0m         timeout\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Handled in the respective communication stack\u001b[39;49;00m\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    339\u001b[0m \u001b[39m# Gather results\u001b[39;00m\n\u001b[1;32m    340\u001b[0m results: List[Tuple[ClientProxy, FitRes]] \u001b[39m=\u001b[39m []\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/concurrent/futures/_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    305\u001b[0m     waiter \u001b[39m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 307\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fs:\n\u001b[1;32m    309\u001b[0m     \u001b[39mwith\u001b[39;00m f\u001b[39m.\u001b[39m_condition:\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Create an instance of the model and get the parameters\n",
        "params = get_parameters(Net())\n",
        "\n",
        "# Pass parameters to the Strategy for server-side parameter initialization\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=0.3,\n",
        "    fraction_evaluate=0.3,\n",
        "    min_fit_clients=3,\n",
        "    min_evaluate_clients=3,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
        ")\n",
        "\n",
        "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "client_resources = None\n",
        "if DEVICE.type == \"cuda\":\n",
        "    client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "# Start simulation\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTCh290fvIGU"
      },
      "source": [
        "Passing `initial_parameters` to the `FedAvg` strategy prevents Flower from asking one of the clients for the initial parameters. If we look closely, we can see that the logs do not show any calls to the `FlowerClient.get_parameters` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rbuaUwfvIGU"
      },
      "source": [
        "### Starting with a customized strategy\n",
        "\n",
        "We've seen the function `start_simulation` before. It accepts a number of arguments, amongst them the `client_fn` used to create `FlowerClient` instances, the number of clients to simulate `num_clients`, the number of rounds `num_rounds`, and the strategy.\n",
        "\n",
        "The strategy encapsulates the federated learning approach/algorithm, for example, `FedAvg` or `FedAdagrad`. Let's try to use a different strategy this time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W37plfF7vIGU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-05-02 07:57:12,285 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
            "2023-05-02 07:57:16,362\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "INFO flwr 2023-05-02 07:57:16,905 | app.py:180 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 16.0, 'memory': 75102343783.0, 'object_store_memory': 36472433049.0, 'node:172.17.0.2': 1.0, 'accelerator_type:RTX': 1.0}\n",
            "INFO flwr 2023-05-02 07:57:16,906 | server.py:86 | Initializing global parameters\n",
            "INFO flwr 2023-05-02 07:57:16,906 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO flwr 2023-05-02 07:57:16,906 | server.py:88 | Evaluating initial parameters\n",
            "INFO flwr 2023-05-02 07:57:16,907 | server.py:101 | FL starting\n",
            "DEBUG flwr 2023-05-02 07:57:16,907 | server.py:218 | fit_round 1: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560097)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560097)\u001b[0m Epoch 1: train loss 0.06533655524253845, accuracy 0.21933333333333332\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560159)\u001b[0m [Client 6] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560159)\u001b[0m Epoch 1: train loss 0.06568127870559692, accuracy 0.2091111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560251)\u001b[0m [Client 3] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 07:57:23,894 | server.py:232 | fit_round 1 received 3 results and 0 failures\n",
            "WARNING flwr 2023-05-02 07:57:23,898 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-05-02 07:57:23,900 | server.py:168 | evaluate_round 1: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560251)\u001b[0m Epoch 1: train loss 0.06523174792528152, accuracy 0.2071111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1560319)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1560384)\u001b[0m [Client 6] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 07:57:29,029 | server.py:182 | evaluate_round 1 received 3 results and 0 failures\n",
            "WARNING flwr 2023-05-02 07:57:29,030 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-05-02 07:57:29,030 | server.py:218 | fit_round 2: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1560445)\u001b[0m [Client 4] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560508)\u001b[0m [Client 7] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560508)\u001b[0m Epoch 1: train loss 0.8384279012680054, accuracy 0.2628888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560575)\u001b[0m [Client 4] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560575)\u001b[0m Epoch 1: train loss 0.809805154800415, accuracy 0.25022222222222223\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560638)\u001b[0m [Client 5] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 07:57:35,961 | server.py:232 | fit_round 2 received 3 results and 0 failures\n",
            "DEBUG flwr 2023-05-02 07:57:35,966 | server.py:168 | evaluate_round 2: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560638)\u001b[0m Epoch 1: train loss 0.8109397888183594, accuracy 0.24288888888888888\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1560709)\u001b[0m [Client 9] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1560770)\u001b[0m [Client 7] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 07:57:41,078 | server.py:182 | evaluate_round 2 received 3 results and 0 failures\n",
            "DEBUG flwr 2023-05-02 07:57:41,079 | server.py:218 | fit_round 3: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1560831)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560898)\u001b[0m [Client 9] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560898)\u001b[0m Epoch 1: train loss 0.11360079050064087, accuracy 0.13844444444444445\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560962)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1560962)\u001b[0m Epoch 1: train loss 0.11138822883367538, accuracy 0.15155555555555555\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1561028)\u001b[0m [Client 8] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 07:57:48,067 | server.py:232 | fit_round 3 received 3 results and 0 failures\n",
            "DEBUG flwr 2023-05-02 07:57:48,072 | server.py:168 | evaluate_round 3: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1561028)\u001b[0m Epoch 1: train loss 0.11405691504478455, accuracy 0.1411111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1561095)\u001b[0m [Client 2] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1561156)\u001b[0m [Client 3] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 07:57:53,281 | server.py:182 | evaluate_round 3 received 3 results and 0 failures\n",
            "INFO flwr 2023-05-02 07:57:53,282 | server.py:147 | FL finished in 36.37512519804295\n",
            "INFO flwr 2023-05-02 07:57:53,282 | app.py:218 | app_fit: losses_distributed [(1, 5.935198771158855), (2, 0.7092998193105062), (3, 0.08116747140884399)]\n",
            "INFO flwr 2023-05-02 07:57:53,282 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-05-02 07:57:53,283 | app.py:220 | app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-05-02 07:57:53,283 | app.py:221 | app_fit: losses_centralized []\n",
            "INFO flwr 2023-05-02 07:57:53,283 | app.py:222 | app_fit: metrics_centralized {}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1561217)\u001b[0m [Client 5] evaluate, config: {}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "History (loss, distributed):\n",
              "\tround 1: 5.935198771158855\n",
              "\tround 2: 0.7092998193105062\n",
              "\tround 3: 0.08116747140884399"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create FedAdam strategy\n",
        "strategy = fl.server.strategy.FedAdagrad(\n",
        "    fraction_fit=0.3,\n",
        "    fraction_evaluate=0.3,\n",
        "    min_fit_clients=3,\n",
        "    min_evaluate_clients=3,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
        ")\n",
        "\n",
        "# Start simulation\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G71YHOQvvIGV"
      },
      "source": [
        "## Server-side parameter **evaluation**\n",
        "\n",
        "Flower can evaluate the aggregated model on the server-side or on the client-side. Client-side and server-side evaluation are similar in some ways, but different in others.\n",
        "\n",
        "**Centralized Evaluation** (or *server-side evaluation*) is conceptually simple: it works the same way that evaluation in centralized machine learning does. If there is a server-side dataset that can be used for evaluation purposes, then that's great. We can evaluate the newly aggregated model after each round of training without having to send the model to clients. We're also fortunate in the sense that our entire evaluation dataset is available at all times.\n",
        "\n",
        "**Federated Evaluation** (or *client-side evaluation*) is more complex, but also more powerful: it doesn't require a centralized dataset and allows us to evaluate models over a larger set of data, which often yields more realistic evaluation results. In fact, many scenarios require us to use **Federated Evaluation** if we want to get representative evaluation results at all. But this power comes at a cost: once we start to evaluate on the client side, we should be aware that our evaluation dataset can change over consecutive rounds of learning if those clients are not always available. Moreover, the dataset held by each client can also change over consecutive rounds. This can lead to evaluation results that are not stable, so even if we would not change the model, we'd see our evaluation results fluctuate over consecutive rounds.\n",
        "\n",
        "이게 뭔소리야;;\n",
        "\n",
        "We've seen how federated evaluation works on the client side (i.e., by implementing the `evaluate` method in `FlowerClient`). Now let's see how we can evaluate aggregated model parameters on the server-side:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QW9lbt-9vIGV"
      },
      "outputs": [],
      "source": [
        "# The `evaluate` function will be by Flower called after every round\n",
        "def evaluate(\n",
        "    server_round: int,\n",
        "    parameters: fl.common.NDArrays,\n",
        "    config: Dict[str, fl.common.Scalar],\n",
        ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
        "    net = Net().to(DEVICE)\n",
        "    valloader = valloaders[0]\n",
        "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
        "    loss, accuracy = test(net, valloader)\n",
        "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
        "    return loss, {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "client_resources = None\n",
        "if DEVICE.type == \"cuda\":\n",
        "    client_resources = {\"num_gpus\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sj1Gc6WbvIGV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-05-04 11:34:29,979 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
            "2023-05-04 11:34:33,886\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "INFO flwr 2023-05-04 11:34:34,429 | app.py:180 | Flower VCE: Ray initialized with resources: {'CPU': 16.0, 'GPU': 1.0, 'memory': 31642664142.0, 'node:172.17.0.2': 1.0, 'object_store_memory': 15821332070.0, 'accelerator_type:RTX': 1.0}\n",
            "INFO flwr 2023-05-04 11:34:34,429 | server.py:86 | Initializing global parameters\n",
            "INFO flwr 2023-05-04 11:34:34,430 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO flwr 2023-05-04 11:34:34,430 | server.py:88 | Evaluating initial parameters\n",
            "INFO flwr 2023-05-04 11:34:34,688 | server.py:91 | initial parameters (loss, other metrics): 0.07383316326141358, {'accuracy': 0.08}\n",
            "INFO flwr 2023-05-04 11:34:34,688 | server.py:101 | FL starting\n",
            "DEBUG flwr 2023-05-04 11:34:34,689 | server.py:218 | fit_round 1: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Server-side evaluation loss 0.07383316326141358 / accuracy 0.08\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=745634)\u001b[0m [Client 7] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=745634)\u001b[0m Epoch 1: train loss 0.06384707987308502, accuracy 0.25466666666666665\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=745751)\u001b[0m [Client 2] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=745751)\u001b[0m Epoch 1: train loss 0.06492054462432861, accuracy 0.22777777777777777\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=745899)\u001b[0m [Client 1] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-04 11:34:42,499 | server.py:232 | fit_round 1 received 3 results and 0 failures\n",
            "WARNING flwr 2023-05-04 11:34:42,503 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
            "INFO flwr 2023-05-04 11:34:42,568 | server.py:119 | fit progress: (1, 0.06464041471481323, {'accuracy': 0.298}, 7.878992336103693)\n",
            "DEBUG flwr 2023-05-04 11:34:42,568 | server.py:168 | evaluate_round 1: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=745899)\u001b[0m Epoch 1: train loss 0.06482955068349838, accuracy 0.23377777777777778\n",
            "Server-side evaluation loss 0.06464041471481323 / accuracy 0.298\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=746036)\u001b[0m [Client 4] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=746149)\u001b[0m [Client 5] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-04 11:34:48,299 | server.py:182 | evaluate_round 1 received 3 results and 0 failures\n",
            "WARNING flwr 2023-05-04 11:34:48,299 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-05-04 11:34:48,300 | server.py:218 | fit_round 2: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=746269)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=746393)\u001b[0m [Client 8] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=746393)\u001b[0m Epoch 1: train loss 0.05693008750677109, accuracy 0.33244444444444443\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=746467)\u001b[0m [Client 6] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=746467)\u001b[0m Epoch 1: train loss 0.057900018990039825, accuracy 0.3253333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=746535)\u001b[0m [Client 7] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-04 11:34:56,137 | server.py:232 | fit_round 2 received 3 results and 0 failures\n",
            "INFO flwr 2023-05-04 11:34:56,206 | server.py:119 | fit progress: (2, 0.05538888573646546, {'accuracy': 0.37}, 21.517673142952845)\n",
            "DEBUG flwr 2023-05-04 11:34:56,207 | server.py:168 | evaluate_round 2: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=746535)\u001b[0m Epoch 1: train loss 0.056419700384140015, accuracy 0.3413333333333333\n",
            "Server-side evaluation loss 0.05538888573646546 / accuracy 0.37\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=746664)\u001b[0m [Client 3] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=746779)\u001b[0m [Client 6] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-04 11:35:01,868 | server.py:182 | evaluate_round 2 received 3 results and 0 failures\n",
            "DEBUG flwr 2023-05-04 11:35:01,869 | server.py:218 | fit_round 3: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=746850)\u001b[0m [Client 5] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=747024)\u001b[0m [Client 8] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=747024)\u001b[0m Epoch 1: train loss 0.052481744438409805, accuracy 0.39466666666666667\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=747096)\u001b[0m [Client 5] fit, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=747096)\u001b[0m Epoch 1: train loss 0.05353112146258354, accuracy 0.366\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=747216)\u001b[0m [Client 9] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-04 11:35:09,842 | server.py:232 | fit_round 3 received 3 results and 0 failures\n",
            "INFO flwr 2023-05-04 11:35:09,910 | server.py:119 | fit progress: (3, 0.05188120603561401, {'accuracy': 0.404}, 35.22159571200609)\n",
            "DEBUG flwr 2023-05-04 11:35:09,911 | server.py:168 | evaluate_round 3: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Server-side evaluation loss 0.05188120603561401 / accuracy 0.404\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=747216)\u001b[0m Epoch 1: train loss 0.053625527769327164, accuracy 0.3748888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=747297)\u001b[0m [Client 5] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=747413)\u001b[0m [Client 9] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-04 11:35:15,774 | server.py:182 | evaluate_round 3 received 3 results and 0 failures\n",
            "INFO flwr 2023-05-04 11:35:15,774 | server.py:147 | FL finished in 41.085737239103764\n",
            "INFO flwr 2023-05-04 11:35:15,775 | app.py:218 | app_fit: losses_distributed [(1, 0.06440572420756023), (2, 0.05476273314158122), (3, 0.05119442224502563)]\n",
            "INFO flwr 2023-05-04 11:35:15,775 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-05-04 11:35:15,775 | app.py:220 | app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-05-04 11:35:15,775 | app.py:221 | app_fit: losses_centralized [(0, 0.07383316326141358), (1, 0.06464041471481323), (2, 0.05538888573646546), (3, 0.05188120603561401)]\n",
            "INFO flwr 2023-05-04 11:35:15,776 | app.py:222 | app_fit: metrics_centralized {'accuracy': [(0, 0.08), (1, 0.298), (2, 0.37), (3, 0.404)]}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=747475)\u001b[0m [Client 8] evaluate, config: {}\n",
            "['accuracy']\n"
          ]
        }
      ],
      "source": [
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=0.3,\n",
        "    fraction_evaluate=0.3,\n",
        "    min_fit_clients=3,\n",
        "    min_evaluate_clients=3,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
        "    evaluate_fn=evaluate,  # Pass the evaluation function\n",
        ")\n",
        "\n",
        "hist = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")\n",
        "\n",
        "\n",
        "metrics_cen = list(hist.metrics_centralized.keys())\n",
        "print(metrics_cen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY_Vs0Q5vIGW"
      },
      "source": [
        "## Sending/receiving arbitrary values to/from clients\n",
        "\n",
        "In some situations, we want to configure client-side execution (trainig, evaluation) from the server-side. One example for that is the server asking the clients to train for a certain number of local epochs. Flower provides a way to send configuration values from the server to the clients using a dictionary. Let's look at an example where the clients receive values from the server through the `config` parameter in `fit` (`config` is also available in `evaluate`). The `fit` method receives the configuration dictionary through the `config` parameter and can then read values from this dictionary. In this example, it reads `server_round` and `local_epochs` and uses those values to improve the logging and configure the number of local training epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MHzH6IzcvIGW"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, net, trainloader, valloader):\n",
        "        self.cid = cid\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        print(f\"[Client {self.cid}] get_parameters\")\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        # Read values from config\n",
        "        server_round = config[\"server_round\"]\n",
        "        local_epochs = config[\"local_epochs\"]\n",
        "\n",
        "        # Use values provided by the config\n",
        "        print(f\"[Client {self.cid}, round {server_round}] fit, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        train(self.net, self.trainloader, epochs=local_epochs)\n",
        "        return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "\n",
        "def client_fn(cid) -> FlowerClient:\n",
        "    net = Net().to(DEVICE)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "    return FlowerClient(cid, net, trainloader, valloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf4RvH05vIGX"
      },
      "source": [
        "So how can we  send this config dictionary from server to clients? The built-in Flower Strategies provide way to do this, and it works similarly to the way server-side evaluation works. We provide a function to the strategy, and the strategy calls this function for every round of federated learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HV2zEI06vIGX"
      },
      "outputs": [],
      "source": [
        "def fit_config(server_round: int):\n",
        "    \"\"\"Return training configuration dict for each round.\n",
        "\n",
        "    Perform two rounds of training with one local epoch, increase to two local\n",
        "    epochs afterwards.\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        \"server_round\": server_round,  # The current round of federated learning\n",
        "        \"local_epochs\": 1 if server_round < 2 else 2,  #\n",
        "    }\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvy0uTpCvIGX"
      },
      "source": [
        "Next, we'll just pass this function to the FedAvg strategy before starting the simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flwr.common import Metrics\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QGRQ1-HQvIGX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-05-02 08:05:26,884 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
            "2023-05-02 08:05:30,912\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "INFO flwr 2023-05-02 08:05:31,453 | app.py:180 | Flower VCE: Ray initialized with resources: {'accelerator_type:RTX': 1.0, 'CPU': 16.0, 'node:172.17.0.2': 1.0, 'object_store_memory': 36463837593.0, 'GPU': 1.0, 'memory': 75082287719.0}\n",
            "INFO flwr 2023-05-02 08:05:31,454 | server.py:86 | Initializing global parameters\n",
            "INFO flwr 2023-05-02 08:05:31,454 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO flwr 2023-05-02 08:05:31,454 | server.py:88 | Evaluating initial parameters\n",
            "INFO flwr 2023-05-02 08:05:31,682 | server.py:91 | initial parameters (loss, other metrics): 0.07375366973876953, {'accuracy': 0.098}\n",
            "INFO flwr 2023-05-02 08:05:31,682 | server.py:101 | FL starting\n",
            "DEBUG flwr 2023-05-02 08:05:31,683 | server.py:218 | fit_round 1: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Server-side evaluation loss 0.07375366973876953 / accuracy 0.098\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587430)\u001b[0m [Client 4, round 1] fit, config: {'server_round': 1, 'local_epochs': 1}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587430)\u001b[0m Epoch 1: train loss 0.06484726816415787, accuracy 0.23\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587494)\u001b[0m [Client 9, round 1] fit, config: {'server_round': 1, 'local_epochs': 1}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587494)\u001b[0m Epoch 1: train loss 0.06577346473932266, accuracy 0.21711111111111112\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587583)\u001b[0m [Client 5, round 1] fit, config: {'server_round': 1, 'local_epochs': 1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:05:38,757 | server.py:232 | fit_round 1 received 3 results and 0 failures\n",
            "WARNING flwr 2023-05-02 08:05:38,761 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
            "INFO flwr 2023-05-02 08:05:38,820 | server.py:119 | fit progress: (1, 0.061814931631088255, {'accuracy': 0.302}, 7.137673989054747)\n",
            "DEBUG flwr 2023-05-02 08:05:38,821 | server.py:168 | evaluate_round 1: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Server-side evaluation loss 0.061814931631088255 / accuracy 0.302\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587583)\u001b[0m Epoch 1: train loss 0.06493223458528519, accuracy 0.21933333333333332\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1587654)\u001b[0m [Client 7] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1587715)\u001b[0m [Client 4] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:05:43,921 | server.py:182 | evaluate_round 1 received 3 results and 0 failures\n",
            "DEBUG flwr 2023-05-02 08:05:43,921 | server.py:218 | fit_round 2: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1587776)\u001b[0m [Client 2] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587842)\u001b[0m [Client 7, round 2] fit, config: {'server_round': 2, 'local_epochs': 2}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587842)\u001b[0m Epoch 1: train loss 0.05631336569786072, accuracy 0.33\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587842)\u001b[0m Epoch 2: train loss 0.05165805667638779, accuracy 0.396\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587907)\u001b[0m [Client 9, round 2] fit, config: {'server_round': 2, 'local_epochs': 2}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587907)\u001b[0m Epoch 1: train loss 0.057463984936475754, accuracy 0.3302222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587907)\u001b[0m Epoch 2: train loss 0.05254611000418663, accuracy 0.3913333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587974)\u001b[0m [Client 4, round 2] fit, config: {'server_round': 2, 'local_epochs': 2}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587974)\u001b[0m Epoch 1: train loss 0.05669064819812775, accuracy 0.33355555555555555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:05:52,889 | server.py:232 | fit_round 2 received 3 results and 0 failures\n",
            "INFO flwr 2023-05-02 08:05:52,951 | server.py:119 | fit progress: (2, 0.05233015179634094, {'accuracy': 0.388}, 21.268996386090294)\n",
            "DEBUG flwr 2023-05-02 08:05:52,952 | server.py:168 | evaluate_round 2: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Server-side evaluation loss 0.05233015179634094 / accuracy 0.388\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1587974)\u001b[0m Epoch 2: train loss 0.05216331034898758, accuracy 0.39244444444444443\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1588044)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1588108)\u001b[0m [Client 6] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:05:58,071 | server.py:182 | evaluate_round 2 received 3 results and 0 failures\n",
            "DEBUG flwr 2023-05-02 08:05:58,072 | server.py:218 | fit_round 3: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1588169)\u001b[0m [Client 3] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588232)\u001b[0m [Client 7, round 3] fit, config: {'server_round': 3, 'local_epochs': 2}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588232)\u001b[0m Epoch 1: train loss 0.04963402450084686, accuracy 0.4242222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588232)\u001b[0m Epoch 2: train loss 0.04628428816795349, accuracy 0.4562222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588300)\u001b[0m [Client 4, round 3] fit, config: {'server_round': 3, 'local_epochs': 2}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588300)\u001b[0m Epoch 1: train loss 0.04971098154783249, accuracy 0.4226666666666667\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588300)\u001b[0m Epoch 2: train loss 0.04688555747270584, accuracy 0.4562222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588364)\u001b[0m [Client 8, round 3] fit, config: {'server_round': 3, 'local_epochs': 2}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588364)\u001b[0m Epoch 1: train loss 0.05027785524725914, accuracy 0.4137777777777778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:06:06,974 | server.py:232 | fit_round 3 received 3 results and 0 failures\n",
            "INFO flwr 2023-05-02 08:06:07,038 | server.py:119 | fit progress: (3, 0.049360291004180906, {'accuracy': 0.432}, 35.355143822031096)\n",
            "DEBUG flwr 2023-05-02 08:06:07,038 | server.py:168 | evaluate_round 3: strategy sampled 3 clients (out of 10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1588364)\u001b[0m Epoch 2: train loss 0.04703766852617264, accuracy 0.4573333333333333\n",
            "Server-side evaluation loss 0.049360291004180906 / accuracy 0.432\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1588437)\u001b[0m [Client 3] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1588498)\u001b[0m [Client 5] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:06:12,315 | server.py:182 | evaluate_round 3 received 3 results and 0 failures\n",
            "INFO flwr 2023-05-02 08:06:12,315 | server.py:147 | FL finished in 40.63267382001504\n",
            "INFO flwr 2023-05-02 08:06:12,316 | app.py:218 | app_fit: losses_distributed [(1, 0.06197010064125061), (2, 0.05191275413831075), (3, 0.04704053934415181)]\n",
            "INFO flwr 2023-05-02 08:06:12,316 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-05-02 08:06:12,316 | app.py:220 | app_fit: metrics_distributed {'accuracy': [(1, 0.3073333333333333), (2, 0.4046666666666667), (3, 0.47266666666666673)]}\n",
            "INFO flwr 2023-05-02 08:06:12,316 | app.py:221 | app_fit: losses_centralized [(0, 0.07375366973876953), (1, 0.061814931631088255), (2, 0.05233015179634094), (3, 0.049360291004180906)]\n",
            "INFO flwr 2023-05-02 08:06:12,316 | app.py:222 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.302), (2, 0.388), (3, 0.432)]}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1588558)\u001b[0m [Client 7] evaluate, config: {}\n"
          ]
        }
      ],
      "source": [
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=0.3,\n",
        "    fraction_evaluate=0.3,\n",
        "    min_fit_clients=3,\n",
        "    min_evaluate_clients=3,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        "    evaluate_fn=evaluate,\n",
        "    on_fit_config_fn=fit_config,  # Pass the fit_config function\n",
        ")\n",
        "\n",
        "histFedAvg = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[(0, 0.098), (1, 0.302), (2, 0.388), (3, 0.432)]]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print(\"\")\n",
        "list(histFedAvg.metrics_centralized.values())\n",
        "\n",
        "# print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c_accuracy</th>\n",
              "      <th>d_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.302</td>\n",
              "      <td>0.307333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.388</td>\n",
              "      <td>0.404667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.432</td>\n",
              "      <td>0.472667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   c_accuracy  d_accuracy\n",
              "0       0.302    0.307333\n",
              "1       0.388    0.404667\n",
              "2       0.432    0.472667"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_result = pd.DataFrame()\n",
        "metrics_cen = list(histFedAvg.metrics_centralized.keys())\n",
        "for metric in metrics_cen:\n",
        "    df_result[f\"c_{metric}\"] = [h[1] for h in histFedAvg.metrics_centralized[metric][1:]]\n",
        "\n",
        "metrics_dis = list(histFedAvg.metrics_distributed.keys())\n",
        "for metric in metrics_dis:\n",
        "    df_result[f\"d_{metric}\"] = [h[1] for h in histFedAvg.metrics_distributed[metric]]\n",
        "\n",
        "df_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quk-ax9GvIGX"
      },
      "source": [
        "As we can see, the client logs now include the current round of federated learning (which they read from the `config` dictionary). We can also configure local training to run for one epoch during the first and second round of federated learning, and then for two epochs during the third round.\n",
        "\n",
        "Clients can also return arbitrary values to the server. To do so, they return a dictionary from `fit` and/or `evaluate`. We have seen and used this concept throughout this notebook without mentioning it explicitly: our `FlowerClient` returns a dictionary containing a custom key/value pair as the third return value in `evaluate`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzghHuE4vIGX"
      },
      "source": [
        "## Scaling federated learning\n",
        "\n",
        "As a last step in this notebook, let's see how we can use Flower to experiment with a large number of clients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tpq2ENSdvIGY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_CLIENTS = 1000\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OekTa12ivIGY"
      },
      "source": [
        "We now have 1000 partitions, each holding 45 training and 5 validation examples. Given that the number of training examples on each client is quite small, we should probably train the model a bit longer, so we configure the clients to perform 3 local training epochs. We should also adjust the fraction of clients selected for training during each round (we don't want all 1000 clients participating in every round), so we adjust `fraction_fit` to `0.05`, which means that only 5% of available clients (so 50 clients) will be selected for training each round:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "o1Kg9m6TvIGY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-05-02 07:59:21,237 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
            "2023-05-02 07:59:25,327\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "INFO flwr 2023-05-02 07:59:25,834 | app.py:180 | Flower VCE: Ray initialized with resources: {'accelerator_type:RTX': 1.0, 'object_store_memory': 36427366809.0, 'CPU': 16.0, 'GPU': 1.0, 'node:172.17.0.2': 1.0, 'memory': 74997189223.0}\n",
            "INFO flwr 2023-05-02 07:59:25,837 | server.py:86 | Initializing global parameters\n",
            "INFO flwr 2023-05-02 07:59:25,837 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO flwr 2023-05-02 07:59:25,837 | server.py:88 | Evaluating initial parameters\n",
            "INFO flwr 2023-05-02 07:59:25,838 | server.py:101 | FL starting\n",
            "DEBUG flwr 2023-05-02 07:59:25,838 | server.py:218 | fit_round 1: strategy sampled 25 clients (out of 1000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567097)\u001b[0m [Client 484, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567097)\u001b[0m Epoch 1: train loss 0.10286653786897659, accuracy 0.022222222222222223\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567097)\u001b[0m Epoch 2: train loss 0.10202530026435852, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567097)\u001b[0m Epoch 3: train loss 0.1010870411992073, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567181)\u001b[0m [Client 714, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567181)\u001b[0m Epoch 1: train loss 0.1024223044514656, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567181)\u001b[0m Epoch 2: train loss 0.1011638417840004, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567181)\u001b[0m Epoch 3: train loss 0.10011611878871918, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567243)\u001b[0m [Client 726, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567243)\u001b[0m Epoch 1: train loss 0.10163050889968872, accuracy 0.08888888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567243)\u001b[0m Epoch 2: train loss 0.10080256313085556, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567243)\u001b[0m Epoch 3: train loss 0.10080020129680634, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567307)\u001b[0m [Client 403, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567307)\u001b[0m Epoch 1: train loss 0.1028190404176712, accuracy 0.044444444444444446\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567307)\u001b[0m Epoch 2: train loss 0.10164915025234222, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567307)\u001b[0m Epoch 3: train loss 0.10098279267549515, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567370)\u001b[0m [Client 251, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567370)\u001b[0m Epoch 1: train loss 0.10258259624242783, accuracy 0.06666666666666667\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567370)\u001b[0m Epoch 2: train loss 0.10150150954723358, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567370)\u001b[0m Epoch 3: train loss 0.10139898210763931, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567432)\u001b[0m [Client 170, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567432)\u001b[0m Epoch 1: train loss 0.10254116356372833, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567432)\u001b[0m Epoch 2: train loss 0.10157651454210281, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567432)\u001b[0m Epoch 3: train loss 0.1015380322933197, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567500)\u001b[0m [Client 277, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567500)\u001b[0m Epoch 1: train loss 0.10206994414329529, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567500)\u001b[0m Epoch 2: train loss 0.10163331031799316, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567500)\u001b[0m Epoch 3: train loss 0.10074632614850998, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567563)\u001b[0m [Client 819, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567563)\u001b[0m Epoch 1: train loss 0.10220266878604889, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567563)\u001b[0m Epoch 2: train loss 0.10167808830738068, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567563)\u001b[0m Epoch 3: train loss 0.10069204121828079, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567628)\u001b[0m [Client 782, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567628)\u001b[0m Epoch 1: train loss 0.10243121534585953, accuracy 0.0\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567628)\u001b[0m Epoch 2: train loss 0.10097270458936691, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567628)\u001b[0m Epoch 3: train loss 0.10000727325677872, accuracy 0.26666666666666666\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567691)\u001b[0m [Client 586, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567691)\u001b[0m Epoch 1: train loss 0.10220258682966232, accuracy 0.08888888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567691)\u001b[0m Epoch 2: train loss 0.1010122299194336, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567691)\u001b[0m Epoch 3: train loss 0.10019776970148087, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567756)\u001b[0m [Client 236, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567756)\u001b[0m Epoch 1: train loss 0.10206294804811478, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567756)\u001b[0m Epoch 2: train loss 0.10118948668241501, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567756)\u001b[0m Epoch 3: train loss 0.10020756721496582, accuracy 0.4222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567819)\u001b[0m [Client 155, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567819)\u001b[0m Epoch 1: train loss 0.10283612459897995, accuracy 0.022222222222222223\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567819)\u001b[0m Epoch 2: train loss 0.10216956585645676, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567819)\u001b[0m Epoch 3: train loss 0.10164390504360199, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567883)\u001b[0m [Client 190, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567883)\u001b[0m Epoch 1: train loss 0.10257060080766678, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567883)\u001b[0m Epoch 2: train loss 0.10183052718639374, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567883)\u001b[0m Epoch 3: train loss 0.1013999730348587, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567946)\u001b[0m [Client 640, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567946)\u001b[0m Epoch 1: train loss 0.10299282521009445, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567946)\u001b[0m Epoch 2: train loss 0.10153396427631378, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1567946)\u001b[0m Epoch 3: train loss 0.10073477774858475, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[1m\u001b[36m(autoscaler +3m17s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
            "\u001b[2m\u001b[1m\u001b[33m(autoscaler +3m17s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568012)\u001b[0m [Client 285, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568012)\u001b[0m Epoch 1: train loss 0.10221625864505768, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568012)\u001b[0m Epoch 2: train loss 0.10128763318061829, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568012)\u001b[0m Epoch 3: train loss 0.09999403357505798, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568077)\u001b[0m [Client 987, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568077)\u001b[0m Epoch 1: train loss 0.10245359688997269, accuracy 0.08888888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568077)\u001b[0m Epoch 2: train loss 0.1015348806977272, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568077)\u001b[0m Epoch 3: train loss 0.10093867033720016, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568141)\u001b[0m [Client 679, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568141)\u001b[0m Epoch 1: train loss 0.10220898687839508, accuracy 0.06666666666666667\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568141)\u001b[0m Epoch 2: train loss 0.1009596511721611, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568141)\u001b[0m Epoch 3: train loss 0.1004197746515274, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568201)\u001b[0m [Client 956, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568201)\u001b[0m Epoch 1: train loss 0.1025346964597702, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568201)\u001b[0m Epoch 2: train loss 0.10187134146690369, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568201)\u001b[0m Epoch 3: train loss 0.10114114731550217, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568263)\u001b[0m [Client 288, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568263)\u001b[0m Epoch 1: train loss 0.10198845714330673, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568263)\u001b[0m Epoch 2: train loss 0.10125157982110977, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568263)\u001b[0m Epoch 3: train loss 0.10030485689640045, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568323)\u001b[0m [Client 218, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568323)\u001b[0m Epoch 1: train loss 0.10250174254179001, accuracy 0.06666666666666667\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568323)\u001b[0m Epoch 2: train loss 0.10182886570692062, accuracy 0.28888888888888886\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568323)\u001b[0m Epoch 3: train loss 0.10022065788507462, accuracy 0.3111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568382)\u001b[0m [Client 219, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568382)\u001b[0m Epoch 1: train loss 0.10274896025657654, accuracy 0.044444444444444446\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568382)\u001b[0m Epoch 2: train loss 0.1018761545419693, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568382)\u001b[0m Epoch 3: train loss 0.10110478103160858, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568441)\u001b[0m [Client 573, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568441)\u001b[0m Epoch 1: train loss 0.1019836887717247, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568441)\u001b[0m Epoch 2: train loss 0.10120546817779541, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568441)\u001b[0m Epoch 3: train loss 0.10037494450807571, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568503)\u001b[0m [Client 650, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568503)\u001b[0m Epoch 1: train loss 0.1024789810180664, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568503)\u001b[0m Epoch 2: train loss 0.10145982354879379, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568503)\u001b[0m Epoch 3: train loss 0.10106240957975388, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568563)\u001b[0m [Client 320, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568563)\u001b[0m Epoch 1: train loss 0.10249121487140656, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568563)\u001b[0m Epoch 2: train loss 0.10145147144794464, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568563)\u001b[0m Epoch 3: train loss 0.10141950100660324, accuracy 0.26666666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:00:03,505 | server.py:232 | fit_round 1 received 25 results and 0 failures\n",
            "WARNING flwr 2023-05-02 08:00:03,534 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-05-02 08:00:03,535 | server.py:168 | evaluate_round 1: strategy sampled 50 clients (out of 1000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568622)\u001b[0m [Client 739, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568622)\u001b[0m Epoch 1: train loss 0.10255886614322662, accuracy 0.08888888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568622)\u001b[0m Epoch 2: train loss 0.10162127763032913, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1568622)\u001b[0m Epoch 3: train loss 0.10058248788118362, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1568695)\u001b[0m [Client 342] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1568758)\u001b[0m [Client 921] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1568816)\u001b[0m [Client 419] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1568874)\u001b[0m [Client 445] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1568932)\u001b[0m [Client 752] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1568993)\u001b[0m [Client 577] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569051)\u001b[0m [Client 153] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569109)\u001b[0m [Client 645] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569166)\u001b[0m [Client 618] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569223)\u001b[0m [Client 704] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569284)\u001b[0m [Client 958] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569341)\u001b[0m [Client 430] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569398)\u001b[0m [Client 634] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569455)\u001b[0m [Client 349] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569517)\u001b[0m [Client 791] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569574)\u001b[0m [Client 429] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569631)\u001b[0m [Client 13] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569691)\u001b[0m [Client 388] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569749)\u001b[0m [Client 209] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569806)\u001b[0m [Client 908] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569863)\u001b[0m [Client 782] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569923)\u001b[0m [Client 235] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1569981)\u001b[0m [Client 777] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570038)\u001b[0m [Client 844] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570095)\u001b[0m [Client 940] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570155)\u001b[0m [Client 306] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570213)\u001b[0m [Client 703] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570270)\u001b[0m [Client 596] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570327)\u001b[0m [Client 240] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570387)\u001b[0m [Client 723] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570445)\u001b[0m [Client 929] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570502)\u001b[0m [Client 356] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570559)\u001b[0m [Client 126] evaluate, config: {}\n",
            "\u001b[2m\u001b[1m\u001b[33m(autoscaler +4m17s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570619)\u001b[0m [Client 528] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570677)\u001b[0m [Client 401] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570734)\u001b[0m [Client 239] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570791)\u001b[0m [Client 911] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570848)\u001b[0m [Client 633] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570909)\u001b[0m [Client 619] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1570966)\u001b[0m [Client 655] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571023)\u001b[0m [Client 403] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571080)\u001b[0m [Client 451] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571141)\u001b[0m [Client 137] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571198)\u001b[0m [Client 404] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571255)\u001b[0m [Client 56] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571312)\u001b[0m [Client 982] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571373)\u001b[0m [Client 591] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571430)\u001b[0m [Client 653] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571487)\u001b[0m [Client 11] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:01:11,099 | server.py:182 | evaluate_round 1 received 50 results and 0 failures\n",
            "WARNING flwr 2023-05-02 08:01:11,100 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-05-02 08:01:11,100 | server.py:218 | fit_round 2: strategy sampled 25 clients (out of 1000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1571544)\u001b[0m [Client 959] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571616)\u001b[0m [Client 981, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571616)\u001b[0m Epoch 1: train loss 0.10220684856176376, accuracy 0.08888888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571616)\u001b[0m Epoch 2: train loss 0.10075175017118454, accuracy 0.26666666666666666\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571616)\u001b[0m Epoch 3: train loss 0.09919605404138565, accuracy 0.28888888888888886\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571678)\u001b[0m [Client 33, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571678)\u001b[0m Epoch 1: train loss 0.10285479575395584, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571678)\u001b[0m Epoch 2: train loss 0.10152085870504379, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571678)\u001b[0m Epoch 3: train loss 0.10007794946432114, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571737)\u001b[0m [Client 324, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571737)\u001b[0m Epoch 1: train loss 0.10265282541513443, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571737)\u001b[0m Epoch 2: train loss 0.10138154029846191, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571737)\u001b[0m Epoch 3: train loss 0.09953062236309052, accuracy 0.26666666666666666\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571797)\u001b[0m [Client 277, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571797)\u001b[0m Epoch 1: train loss 0.1024472787976265, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571797)\u001b[0m Epoch 2: train loss 0.10099398344755173, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571797)\u001b[0m Epoch 3: train loss 0.10058510303497314, accuracy 0.3111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571855)\u001b[0m [Client 458, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571855)\u001b[0m Epoch 1: train loss 0.10234277695417404, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571855)\u001b[0m Epoch 2: train loss 0.10048145800828934, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571855)\u001b[0m Epoch 3: train loss 0.09935171902179718, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571918)\u001b[0m [Client 247, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571918)\u001b[0m Epoch 1: train loss 0.10258234292268753, accuracy 0.06666666666666667\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571918)\u001b[0m Epoch 2: train loss 0.10059598833322525, accuracy 0.3111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571918)\u001b[0m Epoch 3: train loss 0.09885922074317932, accuracy 0.3333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571978)\u001b[0m [Client 262, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571978)\u001b[0m Epoch 1: train loss 0.102451853454113, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571978)\u001b[0m Epoch 2: train loss 0.10187053680419922, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1571978)\u001b[0m Epoch 3: train loss 0.1002511978149414, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572039)\u001b[0m [Client 784, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572039)\u001b[0m Epoch 1: train loss 0.10188332945108414, accuracy 0.06666666666666667\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572039)\u001b[0m Epoch 2: train loss 0.10031671077013016, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572039)\u001b[0m Epoch 3: train loss 0.09989594668149948, accuracy 0.28888888888888886\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572099)\u001b[0m [Client 706, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572099)\u001b[0m Epoch 1: train loss 0.10236579179763794, accuracy 0.1111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572099)\u001b[0m Epoch 2: train loss 0.1003003865480423, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572099)\u001b[0m Epoch 3: train loss 0.09977497905492783, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572161)\u001b[0m [Client 700, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572161)\u001b[0m Epoch 1: train loss 0.10227970778942108, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572161)\u001b[0m Epoch 2: train loss 0.10089954733848572, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572161)\u001b[0m Epoch 3: train loss 0.09912999719381332, accuracy 0.26666666666666666\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572221)\u001b[0m [Client 89, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572221)\u001b[0m Epoch 1: train loss 0.10176713019609451, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572221)\u001b[0m Epoch 2: train loss 0.09986542910337448, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572221)\u001b[0m Epoch 3: train loss 0.09868606925010681, accuracy 0.3111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572280)\u001b[0m [Client 326, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572280)\u001b[0m Epoch 1: train loss 0.1023586094379425, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572280)\u001b[0m Epoch 2: train loss 0.10095612704753876, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572280)\u001b[0m Epoch 3: train loss 0.10003054141998291, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572340)\u001b[0m [Client 204, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572340)\u001b[0m Epoch 1: train loss 0.10256914049386978, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572340)\u001b[0m Epoch 2: train loss 0.10151822865009308, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572340)\u001b[0m Epoch 3: train loss 0.10045968741178513, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572402)\u001b[0m [Client 4, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572402)\u001b[0m Epoch 1: train loss 0.10232793539762497, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572402)\u001b[0m Epoch 2: train loss 0.10124630481004715, accuracy 0.3111111111111111\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572402)\u001b[0m Epoch 3: train loss 0.1012180745601654, accuracy 0.28888888888888886\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572461)\u001b[0m [Client 705, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572461)\u001b[0m Epoch 1: train loss 0.10206029564142227, accuracy 0.044444444444444446\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572461)\u001b[0m Epoch 2: train loss 0.10065053403377533, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572461)\u001b[0m Epoch 3: train loss 0.09997904300689697, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572519)\u001b[0m [Client 911, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572519)\u001b[0m Epoch 1: train loss 0.10228244215250015, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572519)\u001b[0m Epoch 2: train loss 0.10177112370729446, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572519)\u001b[0m Epoch 3: train loss 0.1004190668463707, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572582)\u001b[0m [Client 530, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572582)\u001b[0m Epoch 1: train loss 0.10230414569377899, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572582)\u001b[0m Epoch 2: train loss 0.10108177363872528, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572582)\u001b[0m Epoch 3: train loss 0.10018102079629898, accuracy 0.28888888888888886\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572641)\u001b[0m [Client 209, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572641)\u001b[0m Epoch 1: train loss 0.10153145343065262, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572641)\u001b[0m Epoch 2: train loss 0.09994062781333923, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572641)\u001b[0m Epoch 3: train loss 0.09708049893379211, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572701)\u001b[0m [Client 946, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572701)\u001b[0m Epoch 1: train loss 0.10234387964010239, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572701)\u001b[0m Epoch 2: train loss 0.10048763453960419, accuracy 0.2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572701)\u001b[0m Epoch 3: train loss 0.10009840875864029, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572761)\u001b[0m [Client 110, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572761)\u001b[0m Epoch 1: train loss 0.10119171440601349, accuracy 0.17777777777777778\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572761)\u001b[0m Epoch 2: train loss 0.0998610183596611, accuracy 0.28888888888888886\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572761)\u001b[0m Epoch 3: train loss 0.09766746312379837, accuracy 0.28888888888888886\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572825)\u001b[0m [Client 41, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572825)\u001b[0m Epoch 1: train loss 0.10228803753852844, accuracy 0.13333333333333333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572825)\u001b[0m Epoch 2: train loss 0.10080361366271973, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572825)\u001b[0m Epoch 3: train loss 0.09929829835891724, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572883)\u001b[0m [Client 727, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572883)\u001b[0m Epoch 1: train loss 0.10136546194553375, accuracy 0.15555555555555556\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572883)\u001b[0m Epoch 2: train loss 0.09982815384864807, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572883)\u001b[0m Epoch 3: train loss 0.09737104177474976, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572942)\u001b[0m [Client 649, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572942)\u001b[0m Epoch 1: train loss 0.10272058099508286, accuracy 0.08888888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572942)\u001b[0m Epoch 2: train loss 0.1015472412109375, accuracy 0.24444444444444444\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1572942)\u001b[0m Epoch 3: train loss 0.10101529210805893, accuracy 0.26666666666666666\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573001)\u001b[0m [Client 205, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573001)\u001b[0m Epoch 1: train loss 0.10321167856454849, accuracy 0.08888888888888889\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573001)\u001b[0m Epoch 2: train loss 0.10180613398551941, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573001)\u001b[0m Epoch 3: train loss 0.10069821774959564, accuracy 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-05-02 08:01:47,547 | server.py:232 | fit_round 2 received 25 results and 0 failures\n",
            "DEBUG flwr 2023-05-02 08:01:47,574 | server.py:168 | evaluate_round 2: strategy sampled 50 clients (out of 1000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573064)\u001b[0m [Client 617, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573064)\u001b[0m Epoch 1: train loss 0.10162679851055145, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573064)\u001b[0m Epoch 2: train loss 0.10014112293720245, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1573064)\u001b[0m Epoch 3: train loss 0.09941364079713821, accuracy 0.2222222222222222\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573141)\u001b[0m [Client 793] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573199)\u001b[0m [Client 816] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573256)\u001b[0m [Client 13] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573314)\u001b[0m [Client 449] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573377)\u001b[0m [Client 12] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573434)\u001b[0m [Client 439] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573492)\u001b[0m [Client 397] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573551)\u001b[0m [Client 609] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573613)\u001b[0m [Client 349] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573670)\u001b[0m [Client 556] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573727)\u001b[0m [Client 490] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573784)\u001b[0m [Client 136] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573844)\u001b[0m [Client 78] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573901)\u001b[0m [Client 104] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1573958)\u001b[0m [Client 456] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574016)\u001b[0m [Client 880] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574076)\u001b[0m [Client 524] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574133)\u001b[0m [Client 653] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574190)\u001b[0m [Client 595] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574247)\u001b[0m [Client 305] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574308)\u001b[0m [Client 376] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574365)\u001b[0m [Client 839] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574422)\u001b[0m [Client 869] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574480)\u001b[0m [Client 161] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574540)\u001b[0m [Client 674] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574597)\u001b[0m [Client 807] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574654)\u001b[0m [Client 894] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574712)\u001b[0m [Client 141] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574772)\u001b[0m [Client 835] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574829)\u001b[0m [Client 980] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574886)\u001b[0m [Client 798] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1574944)\u001b[0m [Client 954] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575004)\u001b[0m [Client 108] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575061)\u001b[0m [Client 435] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575118)\u001b[0m [Client 367] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575176)\u001b[0m [Client 169] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575233)\u001b[0m [Client 370] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575290)\u001b[0m [Client 326] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575347)\u001b[0m [Client 106] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575405)\u001b[0m [Client 497] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575462)\u001b[0m [Client 665] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575519)\u001b[0m [Client 559] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575576)\u001b[0m [Client 358] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575634)\u001b[0m [Client 58] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575694)\u001b[0m [Client 335] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575751)\u001b[0m [Client 160] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575808)\u001b[0m [Client 2] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575866)\u001b[0m [Client 583] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575926)\u001b[0m [Client 753] evaluate, config: {}\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1575983)\u001b[0m [Client 427] evaluate, config: {}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 19\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m config\n\u001b[1;32m      9\u001b[0m strategy \u001b[39m=\u001b[39m fl\u001b[39m.\u001b[39mserver\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mFedAvg(\n\u001b[1;32m     10\u001b[0m     fraction_fit\u001b[39m=\u001b[39m\u001b[39m0.025\u001b[39m,  \u001b[39m# Train on 25 clients (each round)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     fraction_evaluate\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m,  \u001b[39m# Evaluate on 50 clients (each round)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     on_fit_config_fn\u001b[39m=\u001b[39mfit_config,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m fl\u001b[39m.\u001b[39;49msimulation\u001b[39m.\u001b[39;49mstart_simulation(\n\u001b[1;32m     20\u001b[0m     client_fn\u001b[39m=\u001b[39;49mclient_fn,\n\u001b[1;32m     21\u001b[0m     num_clients\u001b[39m=\u001b[39;49mNUM_CLIENTS,\n\u001b[1;32m     22\u001b[0m     config\u001b[39m=\u001b[39;49mfl\u001b[39m.\u001b[39;49mserver\u001b[39m.\u001b[39;49mServerConfig(num_rounds\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m),  \u001b[39m# Just three rounds\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     24\u001b[0m     client_resources\u001b[39m=\u001b[39;49mclient_resources,\n\u001b[1;32m     25\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/simulation/app.py:197\u001b[0m, in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised)\u001b[0m\n\u001b[1;32m    194\u001b[0m     initialized_server\u001b[39m.\u001b[39mclient_manager()\u001b[39m.\u001b[39mregister(client\u001b[39m=\u001b[39mclient_proxy)\n\u001b[1;32m    196\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m hist \u001b[39m=\u001b[39m _fl(\n\u001b[1;32m    198\u001b[0m     server\u001b[39m=\u001b[39;49minitialized_server,\n\u001b[1;32m    199\u001b[0m     config\u001b[39m=\u001b[39;49minitialized_config,\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m event(EventType\u001b[39m.\u001b[39mSTART_SIMULATION_LEAVE)\n\u001b[1;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m hist\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/app.py:217\u001b[0m, in \u001b[0;36m_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fl\u001b[39m(\n\u001b[1;32m    213\u001b[0m     server: Server,\n\u001b[1;32m    214\u001b[0m     config: ServerConfig,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m History:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# Fit model\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     hist \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49mfit(num_rounds\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rounds, timeout\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mround_timeout)\n\u001b[1;32m    218\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: losses_distributed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mlosses_distributed))\n\u001b[1;32m    219\u001b[0m     log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: metrics_distributed_fit \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mmetrics_distributed_fit))\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/server.py:133\u001b[0m, in \u001b[0;36mServer.fit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m     history\u001b[39m.\u001b[39madd_metrics_centralized(\n\u001b[1;32m    129\u001b[0m         server_round\u001b[39m=\u001b[39mcurrent_round, metrics\u001b[39m=\u001b[39mmetrics_cen\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    132\u001b[0m \u001b[39m# Evaluate model on a sample of available clients\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m res_fed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_round(server_round\u001b[39m=\u001b[39;49mcurrent_round, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m res_fed:\n\u001b[1;32m    135\u001b[0m     loss_fed, evaluate_metrics_fed, _ \u001b[39m=\u001b[39m res_fed\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/server.py:177\u001b[0m, in \u001b[0;36mServer.evaluate_round\u001b[0;34m(self, server_round, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m log(\n\u001b[1;32m    169\u001b[0m     DEBUG,\n\u001b[1;32m    170\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluate_round \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: strategy sampled \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m clients (out of \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client_manager\u001b[39m.\u001b[39mnum_available(),\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    176\u001b[0m \u001b[39m# Collect `evaluate` results from all clients participating in this round\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m results, failures \u001b[39m=\u001b[39m evaluate_clients(\n\u001b[1;32m    178\u001b[0m     client_instructions,\n\u001b[1;32m    179\u001b[0m     max_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_workers,\n\u001b[1;32m    180\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    181\u001b[0m )\n\u001b[1;32m    182\u001b[0m log(\n\u001b[1;32m    183\u001b[0m     DEBUG,\n\u001b[1;32m    184\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluate_round \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m received \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m results and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m failures\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[39mlen\u001b[39m(failures),\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[39m# Aggregate the evaluation results\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/site-packages/flwr/server/server.py:394\u001b[0m, in \u001b[0;36mevaluate_clients\u001b[0;34m(client_instructions, max_workers, timeout)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mmax_workers) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m    390\u001b[0m     submitted_fs \u001b[39m=\u001b[39m {\n\u001b[1;32m    391\u001b[0m         executor\u001b[39m.\u001b[39msubmit(evaluate_client, client_proxy, ins, timeout)\n\u001b[1;32m    392\u001b[0m         \u001b[39mfor\u001b[39;00m client_proxy, ins \u001b[39min\u001b[39;00m client_instructions\n\u001b[1;32m    393\u001b[0m     }\n\u001b[0;32m--> 394\u001b[0m     finished_fs, _ \u001b[39m=\u001b[39m concurrent\u001b[39m.\u001b[39;49mfutures\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    395\u001b[0m         fs\u001b[39m=\u001b[39;49msubmitted_fs,\n\u001b[1;32m    396\u001b[0m         timeout\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Handled in the respective communication stack\u001b[39;49;00m\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[39m# Gather results\u001b[39;00m\n\u001b[1;32m    400\u001b[0m results: List[Tuple[ClientProxy, EvaluateRes]] \u001b[39m=\u001b[39m []\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/concurrent/futures/_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    305\u001b[0m     waiter \u001b[39m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 307\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fs:\n\u001b[1;32m    309\u001b[0m     \u001b[39mwith\u001b[39;00m f\u001b[39m.\u001b[39m_condition:\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m~/anaconda3/envs/fl/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def fit_config(server_round: int):\n",
        "    config = {\n",
        "        \"server_round\": server_round,\n",
        "        \"local_epochs\": 3,\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=0.025,  # Train on 25 clients (each round)\n",
        "    fraction_evaluate=0.05,  # Evaluate on 50 clients (each round)\n",
        "    min_fit_clients=20,\n",
        "    min_evaluate_clients=40,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
        "    on_fit_config_fn=fit_config,\n",
        ")\n",
        "\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfnk5yd7vIGY"
      },
      "source": [
        "## Recap\n",
        "\n",
        "In this notebook, we've seen how we can gradually enhance our system by customizing the strategy, initializing parameters on the server side, choosing a different strategy, and evaluating models on the server-side. That's quite a bit of flexibility with so little code, right?\n",
        "\n",
        "In the later sections, we've seen how we can communicate arbitrary values between server and clients to fully customize client-side execution. With that capability, we built a large-scale Federated Learning simulation using the Flower Virtual Client Engine and ran an experiment involving 1000 clients in the same workload - all in a Jupyter Notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aMKjiGGvIGY"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Before you continue, make sure to join the Flower community on Slack: [Join Slack](https://flower.dev/join-slack/)\n",
        "\n",
        "There's a dedicated `#questions` channel if you need help, but we'd also love to hear who you are in `#introductions`!\n",
        "\n",
        "The [Flower Federated Learning Tutorial - Part 3 [WIP]](https://flower.dev/docs/tutorial/Flower-3-Building-a-Strategy-PyTorch.html) shows how to build a fully custom `Strategy` from scratch."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 64-bit ('flower-3.7.12')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
